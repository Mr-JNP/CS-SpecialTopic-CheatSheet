{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline Cheat Sheet\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "  Data Preprocessing\n",
    "    1. Imputation\n",
    "    2. Outlier Handling\n",
    "    3. Grouping\n",
    "  \n",
    "  Handling Continuous Data\n",
    "    1. Using It raw\n",
    "    2. Using Derived Value\n",
    "    3. Using Count and Frequency\n",
    "    4. Binarization\n",
    "    5. Percentage & Rounding\n",
    "    6. Polynomial Features\n",
    "    7. Binning\n",
    "    8. Statistical Transformation\n",
    "  \n",
    "  Handling Categorical Data\n",
    "    1. Norminal Attributes\n",
    "    2. Ordinal Attributes\n",
    "    3. One-Hot Encoding\n",
    "    4. Bin Counting\n",
    "    5. Feature Hashing\n",
    "  \n",
    "  Feature Selection\n",
    "    1. Correlation Heatmap\n",
    "    2. Univariate Selection using Select K-Best w/ Scoring\n",
    "    3. Recursive Feature Elimination\n",
    "    4. Select from Model\n",
    "  \n",
    "  Handling Imbalanced Data\n",
    "    1. SMOTE (Oversampling)\n",
    "    2. Random Undersampling w/ Ensemble Methods\n",
    "    3. Near Miss (Undersampling)\n",
    "    4. Cost-based Classification\n",
    "    5. Probability Threshold\n",
    "  \n",
    "  Test and Train Spliting\n",
    "    1. Stratification\n",
    "    \n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(),\n",
    "                           inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Handling Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier detection with SD\n",
    "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\n",
    "\n",
    "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean() + data['column'].std() * factor\n",
    "lower_lim = data['column'].mean() - data['column'].std() * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, z-score can be used instead of the formula above.\n",
    "\n",
    "Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outlier using Z-score\n",
    "data['z_score'] = data.groupby('column1')['column2'].apply(\n",
    "    lambda x: (x - x.mean()) / x.std())\n",
    "# Plot Density Curve\n",
    "data = data[(data['z_score'] < 3) & (data['z_score'] > -3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier detection with Percentile\n",
    "Another mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier.\n",
    "\n",
    "The key point is here to set the percentage value once again, and this depends on the distribution of your data as mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An Outlier Dilemma: Drop or Cap\n",
    "Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.\n",
    "On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim), column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim), column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Grouping\n",
    "The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivot Table\n",
    "Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group',\n",
    "                 columns='column_to_encode',\n",
    "                 values='aggregation_column',\n",
    "                 aggfunc=np.sum,\n",
    "                 fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Column Grouping\n",
    "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series, you can group data by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm3_s2_d = pm3_s2_d.set_index(pd.DatetimeIndex(pm3_s2_d['timest'])).groupby(\n",
    "    pd.Grouper(freq='d')).mean().dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Handling Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as spstats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Raw Measure\n",
    "Raw measures are typically indicated using numeric variables directly as features without any form of transformation or engineering. Typically these features can indicate values or counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_df = pd.read_csv('datasets/Pokemon.csv', encoding='utf-8') poke_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Clip Value\n",
    "If you closely observe the data frame snapshot in the above figure, you can see that several attributes represent numeric raw values which can be used directly. The following snippet depicts some of these features with more emphasis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_df[['HP', 'Attack', 'Defense']].head()\n",
    "poke_df[['HP', 'Attack', 'Defense']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Counts and Frequency\n",
    "Another form of raw measures include features which represent frequencies, counts or occurrences of specific attributes. Let’s look at a sample of data from the millionsong dataset which depicts counts or frequencies of songs which have been heard by various users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Binarization\n",
    "Often raw frequencies or counts may not be relevant for building a model based on the problem which is being solved. For instance if I’m building a recommendation system for song recommendations, I would just want to know if a person is interested or has listened to a particular song. This doesn’t require the number of times a song has been listened to since I am more concerned about the various songs he\\she has listened to. In this case, a binary feature is preferred as opposed to a count based feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watched = np.array(popsong_df['listen_count']) \n",
    "watched[watched >= 1] = 1\n",
    "popsong_df['watched'] = watched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use scikit learn binarizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "bn = Binarizer(threshold=0.9)\n",
    "pd_watched = bn.transform([popsong_df['listen_count']])[0]\n",
    "popsong_df['pd_watched'] = pd_watched\n",
    "popsong_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Percentage and Rounding\n",
    "Hence it often makes sense to round off these `high precision percentages into numeric integers`. These integers can then be directly used as raw values or even as categorical (discrete-class based) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_popularity = pd.read_csv('datasets/item_popularity.csv', encoding='utf-8')\n",
    "items_popularity['popularity_scale_10'] = np.array(\n",
    "                   np.round((items_popularity['pop_percent'] * 10)),  \n",
    "                   dtype='int')\n",
    "items_popularity['popularity_scale_100'] = np.array(\n",
    "                  np.round((items_popularity['pop_percent'] * 100)),    \n",
    "                  dtype='int')\n",
    "items_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Feature Interaction and Polynomial Features\n",
    "In this case, this simple linear model depicts the relationship between the output and inputs, purely based on the individual, separate input features.\n",
    "\n",
    "However, often in several real-world scenarios, it makes sense to also try and `capture the interactions between these feature variables as a part of the input feature set`. A simple depiction of the extension of the above linear regression formulation with interaction features would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "res = pf.fit_transform(atk_def)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Binning\n",
    "Binning, also known as quantization is used for `transforming continuous numeric features into discrete ones (categories)`. These discrete values or numbers can be thought of as categories or bins into which the raw, continuous numeric values are binned or grouped into. Each bin represents a specific degree of intensity and hence a specific range of continuous numeric values fall into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_band']=0\n",
    "df.loc[df['Age']<=16,'Age_band']=0\n",
    "df.loc[(df['Age']>16)&(df['Age']<=32),'Age_band']=1\n",
    "df.loc[(df['Age']>32)&(df['Age']<=48),'Age_band']=2\n",
    "df.loc[(df['Age']>48)&(df['Age']<=64),'Age_band']=3\n",
    "df.loc[df['Age']>64,'Age_band']=4\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ranges = [0, 15, 30, 45, 60, 75, 100]\n",
    "bin_names = [1, 2, 3, 4, 5, 6]\n",
    "fcc_survey_df['Age_bin_custom_range'] = pd.cut(np.array(\n",
    "                                              fcc_survey_df['Age']), \n",
    "                                              bins=bin_ranges)\n",
    "fcc_survey_df['Age_bin_custom_label'] = pd.cut(np.array(\n",
    "                                              fcc_survey_df['Age']), \n",
    "                                              bins=bin_ranges,            \n",
    "                                              labels=bin_names)\n",
    "# view the binned features \n",
    "fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', \n",
    "               'Age_bin_custom_range',   \n",
    "               'Age_bin_custom_label']].iloc[10a71:1076]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive\n",
    "(you can change the criteria to be z-score or SD instead of quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\n",
    "fcc_survey_df['Income_quantile_range'] = pd.qcut(\n",
    "                                            fcc_survey_df['Income'], \n",
    "                                            q=quantile_list)\n",
    "fcc_survey_df['Income_quantile_label'] = pd.qcut(\n",
    "                                            fcc_survey_df['Income'], \n",
    "                                            q=quantile_list,       \n",
    "                                            labels=quantile_labels)\n",
    "\n",
    "fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range', \n",
    "               'Income_quantile_label']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "for quantile in quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "ax.set_title('Developer Income Histogram with Quantiles', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Developer Income', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn also provide `KBinsDiscretizer` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "binner = KBinsDiscretizer(encode='ordinal')\n",
    "bins = binner.fit_transform(train_df[['sepal.length']])\n",
    "\n",
    "sns.scatterplot(x=train_df['sepal.length'], y=bins[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Statistical Transformation\n",
    "Their main significance is that they help in `stabilizing variance`, adhering closely to the normal distribution and making the data independent of the mean based on its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc_survey_df['Income_log'] = np.log((1+ fcc_survey_df['Income']))\n",
    "fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_log']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_log_mean = np.round(np.mean(fcc_survey_df['Income_log']), 2)\n",
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income_log'].hist(bins=30, color='#A9C5D3', \n",
    "                                 edgecolor='black', grid=False)\n",
    "plt.axvline(income_log_mean, color='r')\n",
    "ax.set_title('Developer Income Histogram after Log Transform', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Developer Income (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.text(11.5, 450, r'$\\mu$='+str(income_log_mean), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `scikit-learn` also has the standardization\n",
    "here are many approaches to standardize the features. Common approaches are\n",
    "\n",
    "* `sklearn.preprocessing.StandardScaler`\n",
    "* `sklearn.preprocessing.RobustScaler`\n",
    "\n",
    "`RobustScaler` uses quartiles instead of mean and variance as in `StandardScaler`, so it is more suitable for data with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_train_df = pd.DataFrame(\n",
    "    scaler.fit_transform(train_df[features]), \n",
    "    index=train_df.index,\n",
    "    columns=features)\n",
    "scaled_train_df['variety'] = train_df['variety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(8, 8))\n",
    "for i, c in enumerate(train_df.columns):\n",
    "    if c == 'variety':\n",
    "        continue\n",
    "    sns.violinplot(y='variety', x=c, data=train_df, ax=axes[i][0])\n",
    "    sns.violinplot(y='variety', x=c, data=scaled_train_df, ax=axes[i][1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norminal Attribute\n",
    "Nominal attributes consist of discrete categorical values with no notion or sense of order amongst them. The idea here is to transform these attributes into a more representative numerical format which can be easily understood by downstream code and pipelines.\n",
    "\n",
    "`scikit-learn` also provide `LabelEncoder` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "gle = LabelEncoder()\n",
    "genre_labels = gle.fit_transform(vg_df['Genre'])\n",
    "genre_mappings = {index: label for index, label in \n",
    "                  enumerate(gle.classes_)}\n",
    "genre_mappings\n",
    "Output\n",
    "------\n",
    "{0: 'Action', 1: 'Adventure', 2: 'Fighting', 3: 'Misc',\n",
    " 4: 'Platform', 5: 'Puzzle', 6: 'Racing', 7: 'Role-Playing',\n",
    " 8: 'Shooter', 9: 'Simulation', 10: 'Sports', 11: 'Strategy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed labels are stored in the `genre_labels` value which we can write back to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_df['GenreLabel'] = genre_labels\n",
    "vg_df[['Name', 'Platform', 'Year', 'Genre', 'GenreLabel']].iloc[1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Ordinal Attribute\n",
    "Ordinal attributes are categorical attributes with a sense of order amongst the values.\n",
    "\n",
    "`scikit-learn` also provide `OrdinallEncoder` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual way\n",
    "gen_ord_map = {'Gen 1': 1, 'Gen 2': 2, 'Gen 3': 3, \n",
    "               'Gen 4': 4, 'Gen 5': 5, 'Gen 6': 6}\n",
    "poke_df['GenerationLabel'] = poke_df['Generation'].map(gen_ord_map)\n",
    "poke_df[['Name', 'Generation', 'GenerationLabel']].iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "gle = OrdinalEncoder()\n",
    "gen_labels = gle.fit_transform(vg_df['Generation'])\n",
    "gen_mappings = {index: label for index, label in \n",
    "                  enumerate(gle.classes_)}\n",
    "gen_mappings\n",
    "Output\n",
    "------\n",
    "{'Gen 1': 1, 'Gen 2': 2, 'Gen 3': 3, 'Gen 4': 4, 'Gen 5': 5, 'Gen 6': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed labels are stored in the `gen_labels` value which we can write back to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_df['GenerationLabel'] = gen_labels\n",
    "vg_df[['Name', 'Platform', 'Year', 'Generation', 'GenerationLabel']].iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### One-hot Encoding Scheme\n",
    "Ordinal attributes are categorical attributes with a sense of order amongst the values.\n",
    "\n",
    "`scikit-learn` also provide `OneHotEncoder` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode generation labels using one-hot encoding scheme\n",
    "gen_ohe = OneHotEncoder()\n",
    "gen_feature_arr = gen_ohe.fit_transform(poke_df[['Gen_Label']]).toarray()\n",
    "gen_feature_labels = list(gen_le.classes_)\n",
    "gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)\n",
    "\n",
    "# encode legendary status labels using one-hot encoding scheme\n",
    "leg_ohe = OneHotEncoder()\n",
    "leg_feature_arr = leg_ohe.fit_transform(poke_df[['Lgnd_Label']]).toarray()\n",
    "leg_feature_labels = ['Legendary_'+str(cls_label) for cls_label in leg_le.classes_]\n",
    "leg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this in `pandas` w/ the `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_onehot_features = pd.get_dummies(poke_df['Generation'])\n",
    "pd.concat([poke_df[['Name', 'Generation']], gen_onehot_features], axis=1).iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Bin-counting Scheme\n",
    "The encoding schemes we discussed so far, work quite well on categorical data in general, but they start causing problems when the number of distinct categories in any feature becomes very large.\n",
    "\n",
    "Besides this, we also have to deal with what is popularly known as the `curse of dimensionality` where basically with an enormous number of features and not enough representative samples, model performance starts getting affected often leading to overfitting.\n",
    "\n",
    "This scheme needs historical data as a pre-requisite and is an elaborate one.\n",
    "\n",
    "You can do this by `pandas.groupby` categories and using count function. Then you bin them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Feature Hashing Scheme\n",
    "The feature hashing scheme is another useful feature engineering scheme for `dealing with large scale categorical features`.\n",
    "\n",
    "Hashing schemes work on strings, numbers and other structures like vectors. You can think of hashed outputs as a finite set of b bins such that when hash function is applied on the same values\\categories, they get assigned to the same bin (or subset of bins) out of the b bins based on the hash value. We can pre-define the value of b which becomes the final size of the encoded feature vector for each categorical attribute that we encode using the feature hashing scheme.\n",
    "\n",
    "Thus even if we have over 1000 distinct categories in a feature and we set b=10 as the final feature vector size, the output feature set will still have only 10 features as compared to 1000 binary features if we used a one-hot encoding scheme. Let’s consider the Genre attribute in our video game dataset.\n",
    "\n",
    "`scikit-learn` also provide `FeatureHasher` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "fh = FeatureHasher(n_features=6, input_type='string')\n",
    "hashed_features = fh.fit_transform(vg_df['Genre'])\n",
    "hashed_features = hashed_features.toarray()\n",
    "pd.concat([vg_df[['Name', 'Genre']], pd.DataFrame(hashed_features)], \n",
    "          axis=1).iloc[1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap\n",
    "Correlation states how the features are related to each other or the target variable.\n",
    "\n",
    "Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the `seaborn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "#get correlations of each features in dataset\n",
    "corrmat = data.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Selection using Select K-Best w/ Scoring\n",
    "The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "There are many statistical methods to measure the dependency between two variables. In this notebook, we will look at a few scoring available in sklearn.\n",
    "\n",
    "Classification\n",
    "* Chi-squared test (`chi2`): often used for fequency variables (count)\n",
    "* One-way ANOVA (`f_classif`): used when variables are approximately normally distributed. Also all groups (by labels) have roughly the same variance.\n",
    "* Mutual Information (`mutual_info_classif`)\n",
    "\n",
    "Regression\n",
    "* Pearson Correlation (`f_regression`): used when variables are approximately normally distributed.\n",
    "* Mutual Information (`mutual_info_regression`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "new_X = bestfeatures.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "sub2 = cross_val_score(DecisionTreeClassifier(), new_X, y, cv=10, error_score='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination (RFE)\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "\n",
    "`RFECV` performs `RFE` in a cross-validation loop to find the optimal number of features.\n",
    "\n",
    "`scikit-learn` provide `RFECV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "# Create the RFE object and rank each pixel\n",
    "clf_rf_3 = RandomForestClassifier()      \n",
    "rfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\n",
    "rfe = rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "clf_rf_4 = RandomForestClassifier() \n",
    "rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select from Model\n",
    "You can get the `feature importance` of each feature of your dataset by using the feature importance property of the model.\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n",
    "\n",
    "`scikit-learn` provide `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree-based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_)\n",
    "\n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select from Model as a part of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.SVm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Handling Imbalanced Data\n",
    "\n",
    "#### Label distribution\n",
    "Another quick check is to see if the labels are uniformly distribution (a fancy way to say that labels have the same frequency). Most machine learning algorithms work well on balance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['variety'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the label distribution is not uniform (or we say imbalance), there are many ways to deal with this problem as well:\n",
    "\n",
    "#### SMOTE\n",
    "SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n",
    "\n",
    "`imblearn` provide `SMOTE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample with SMOTE and random undersample for imbalanced dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, \n",
    "                           n_redundant=0,n_clusters_per_class=1, \n",
    "                           weights=[0.99], flip_y=0, random_state=1)\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dataset\n",
    "X, y = pipeline.fit_resample(X, y)\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = where(y == label)[0]\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree  on imbalanced dataset with SMOTE oversampling and random undersampling\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, \n",
    "                           n_redundant=0, n_clusters_per_class=1, \n",
    "                           weights=[0.99], flip_y=0, random_state=1)\n",
    "\n",
    "# define pipeline\n",
    "model = DecisionTreeClassifier()\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('over', over), ('under', under), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Undersampling w/ Ensemble Methods\n",
    "\n",
    "Undersampling the majority class in the bootstrap is referred to as UnderBagging, and combining both approaches is referred to as OverUnderBagging.\n",
    "\n",
    "The `imbalanced-learn` library provides an implementation of UnderBagging.\n",
    "\n",
    "Specifically, it provides a version of bagging that uses a random undersampling strategy on the majority class within a bootstrap sample in order to balance the two classes. This is provided in the `BalancedBaggingClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagged decision trees with random undersampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, \n",
    "                           n_redundant=0, n_clusters_per_class=1, \n",
    "                           weights=[0.99], flip_y=0, random_state=4)\n",
    "\n",
    "# define model\n",
    "model = BalancedBaggingClassifier()\n",
    "\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Train Spliting\n",
    "\n",
    "Test data = Black Box. Don't touch it. It is meant for testing only.\n",
    "Train data = Do whatever you want with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "columns = “age sex bmi map tc ldl hdl tch ltg glu”.split()\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data, columns=columns)\n",
    "y = diabetes.target\n",
    "\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, Stratify=y)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratification for Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` has a class to do `StratifiedKFold`\n",
    "\n",
    "It forces each fold to have at least m instances of each class. This approach ensures that one class of data is not overrepresented especially when the target variable is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "  Classification\n",
    "    1. Decision Tree Classifier (> 100K Sample)\n",
    "    2. SGD Classifier\n",
    "    3. Logistic Regression\n",
    "    4. KNN (< 100K Sample)\n",
    "    5. Linear SVC\n",
    "    6. SVC (Non-linear Kernel)\n",
    "    7. Ensemble Classifier\n",
    "    8. Naive Bayes (Text Data)\n",
    "    9. Neural Network\n",
    "    \n",
    "  Regression\n",
    "    1. SGD Regressor (> 100K Sample)\n",
    "    2. Linear Model\n",
    "    3. Ridge Regression (< 100K Sample)\n",
    "    4. SVR (Linear Kernel)\n",
    "    5. SVR (Non-linear Kernel)\n",
    "    6. Ensemble Regressor\n",
    "    7. Lasso\n",
    "    8. Elastic Net\n",
    "    9. Neural Network\n",
    "    \n",
    "  Clustering\n",
    "    1. Minibatch K-mean (know K)\n",
    "    2. K-mean \n",
    "    3. Spectral Clustering \n",
    "    4. Gaussian Mixture Modeling\n",
    "    5. Meanshift (Don't know K)\n",
    "    6. DBSCAN\n",
    "    \n",
    "  Dimensionality Reduction\n",
    "    1. Randomized PCA\n",
    "    2. ISO Map\n",
    "    3. Spectral Embedding\n",
    "    4. Local Linear Embedding (LLE)\n",
    "  \n",
    "  Pipeline\n",
    "  \n",
    "  GridSearch CV\n",
    "  \n",
    "  Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "model = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(random_state=0, tol=1e-5)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC w/ Non-linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                   learning_rate=1.0,\n",
    "                                   max_depth=1,\n",
    "                                   random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 5 nodes in hidden layer 1\n",
    "# 2 nodes in hidden layer 2\n",
    "model = MLPClassifier(solver='lbfgs',\n",
    "                      alpha=1e-5,\n",
    "                      hidden_layer_sizes=(5, 2),\n",
    "                      random_state=1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = LinearSVR(random_state=0, tol=1e-5)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR(C=1.0, epsilon=0.2)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "model = BaggingRegressor(base_estimator=SVR(),\n",
    "                         n_estimators=100,\n",
    "                         random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100,\n",
    "                                  learning_rate=0.1,\n",
    "                                  max_depth=1,\n",
    "                                  random_state=0,\n",
    "                                  loss='ls')\n",
    "model.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.Lasso(alpha=0.1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "model = ElasticNetCV(cv=5, random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(solver='sgd',\n",
    "                      alpha=1e-5,\n",
    "                      hidden_layer_sizes=(100, ),\n",
    "                      random_state=1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minibatch K-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "model = MiniBatchKMeans(n_clusters=2,\n",
    "                        random_state=0,\n",
    "                        batch_size=6,\n",
    "                        max_iter=10)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-mean Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=2, random_state=0, max_iter=10)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "model = SpectralClustering(n_clusters=2,\n",
    "                           assign_labels=\"discretize\",\n",
    "                           random_state=0)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GMM\n",
    "\n",
    "model = mixture.GMM(n_components=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = mixture.GaussianMixture(n_components=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanshift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "model = MeanShift(bandwidth=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "model = DBSCAN(eps=3, min_samples=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "model = RandomizedPCA(n_components=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISO Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "embedding = Isomap(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "embedding = SpectralEmbedding(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "embedding = LocallyLinearEmbedding(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2), preprocessing.StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(100, ),\n",
    "                 solver='sgd',\n",
    "                 alpha=0.0001,\n",
    "                 max_iter=100000))\n",
    "\n",
    "scores = cross_validate(model,\n",
    "                        X,\n",
    "                        y,\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        return_train_score=True)\n",
    "\n",
    "a = pd.DataFrame(scores)\n",
    "a['fold'] = a.index\n",
    "a = a.drop(columns=['fit_time', 'score_time'])\n",
    "a['test_score'] = a['test_score'] * -1\n",
    "a['train_score'] = a['train_score'] * -1\n",
    "b = pd.melt(a, id_vars='fold', var_name='split', value_name='score')\n",
    "sns.barplot(x='fold',\n",
    "            y='score',\n",
    "            hue='split',\n",
    "            hue_order=['train_score', 'test_score'],\n",
    "            data=b)\n",
    "__ = plt.title((\n",
    "    f'Train avg MSE = {a[\"train_score\"].mean():.3} +/- {a[\"train_score\"].std() * 2:.3}\\n'\n",
    "    f'Test avg MSE = {a[\"test_score\"].mean():.3} +/- {a[\"test_score\"].std() * 2:.3}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Grid Search\n",
    "gs = GridSearchCV(DecisionTreeClassifier(),\n",
    "                  param_grid={'min_samples_split': range(2, 15, 1)},\n",
    "                  scoring='accuracy',\n",
    "                  refit='AUC',\n",
    "                  return_train_score=True)\n",
    "gs.fit(X, y)\n",
    "results = gs.cv_results_\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize=16)\n",
    "\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_min_samples_split'].data, dtype=float)\n",
    "\n",
    "scorer = 'score'\n",
    "color = 'g'\n",
    "for sample, style in (('train', '--'), ('test', '-')):\n",
    "    sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "    sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "    ax.fill_between(X_axis,\n",
    "                    sample_score_mean - sample_score_std,\n",
    "                    sample_score_mean + sample_score_std,\n",
    "                    alpha=0.1 if sample == 'test' else 0,\n",
    "                    color=color)\n",
    "    ax.plot(X_axis,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=color,\n",
    "            alpha=1 if sample == 'test' else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
